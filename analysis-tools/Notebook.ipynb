{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# For Analysis\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5e0f99",
   "metadata": {},
   "source": [
    "# Aquiring Data\n",
    "\n",
    "This will aquire the data from Drive once fully developed. Currently getting it from the data folder that is being ignored in Github. Right now, we are only getting it from folders Hits and Query, not Assembly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceca7592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will have to change after we set it up to pull from Drive\n",
    "datapath =  os.getcwd() + \"/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our dataframe. Currently hardcoded with the name of the csv.\n",
    "query = pd.read_csv(datapath + \"query.csv\")\n",
    "hits = pd.read_csv(datapath + \"hits.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fe8c3",
   "metadata": {},
   "source": [
    "# Cleaning Data\n",
    "\n",
    "This section cleans the data and also shows exactly what was dropped/manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea112f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are for later exception raising/preliminary analysis of the actual csv structure.\n",
    "# They are mainly for cleaning the data, analysis functions will be later/in a different block.\n",
    "def compare_columns_sets(df1,df2):\n",
    "    set1_c, set2_c = set(df1.columns), set(df2.columns)\n",
    "    # \"a - b\" removes the items in a that it shares with b. (just look up set theory)\n",
    "    df1_dif = set1_c - set2_c\n",
    "    df2_dif = set2_c - set1_c\n",
    "    return df1_dif, df2_dif\n",
    "\n",
    "def drop_nan_columns(df):\n",
    "    dropped_df = df.dropna(axis=1,how='all')\n",
    "    return dropped_df\n",
    "\n",
    "def unusual_row_mask(df, col, threshold=0.5):\n",
    "    \"\"\"Return a Boolean mask for rows where the column has values,\n",
    "    but the column is mostly NaN based on the threshold.\"\"\"\n",
    "    if df[col].isna().mean() >= threshold:\n",
    "        return df[col].notna()\n",
    "    return pd.Series([False] * len(df), index=df.index)\n",
    "\n",
    "def get_peculiar_columns(df,threshold=0.5):\n",
    "    return (df.isnull().sum() / df.shape[0])[df.isnull().sum() / df.shape[0] > threshold]\n",
    "\n",
    "def rows_for_peculiar_columns(df,threshold=0.5):\n",
    "    masks = {}\n",
    "    p_series = get_peculiar_columns(df,threshold)\n",
    "    for p_col in p_series.index:\n",
    "        mask = unusual_row_mask(df, p_col, threshold)\n",
    "        if mask.any():  # Only keep masks that select at least one row. A dictionary\n",
    "            masks[p_col] = mask\n",
    "    return masks\n",
    "\n",
    "def split_query_name(row,splitting_column='Name'):\n",
    "    name = row[splitting_column]\n",
    "    name = name.split()\n",
    "    for item in name:\n",
    "        if \"=\" in item:\n",
    "            item = item.split(\"=\")\n",
    "            if item[1] == '' or item[1] == ' ' or item[1] == []:\n",
    "                item[1] = np.nan\n",
    "            row[item[0]] = item[1]\n",
    "    return row\n",
    "\n",
    "def compare_columns_rowwise(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Compare two columns in a DataFrame row-wise.\n",
    "    Returns:\n",
    "        -1 if all values are the same in every row,\n",
    "        otherwise, a tuple of row indices where values differ.\n",
    "    \"\"\"\n",
    "    mask = df[col1] != df[col2]\n",
    "    diff_indices = tuple(df.index[mask])\n",
    "    return -1 if not diff_indices else diff_indices\n",
    "\n",
    "def combine_col1_into_col2(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Combine values from col1 into col2 if col2 is NaN.\n",
    "    If col2 is not NaN, it will keep the original value.\n",
    "    \"\"\"\n",
    "    df.loc[:,col2] = df.loc[:,col1]\n",
    "    return df.drop(columns=[col1], axis=1)  # Drops col1 after combining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fee1c",
   "metadata": {},
   "source": [
    "### Immediate Cleanup\n",
    "\n",
    "Some of the data has all NaN values for certain columns, so they need to be cleaned up. Also, some qualities of the unclean data are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5593648",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(query.columns), \"columns in uncleaned query\")\n",
    "print(len(hits.columns), \"columns in uncleaned hits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9699f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f343af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deac7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops missing columns and combines same columns.\n",
    "dropped_query = drop_nan_columns(query)\n",
    "dropped_hits = drop_nan_columns(hits)\n",
    "# Error check. Make Better Later\n",
    "\n",
    "print(\"For dropped query\", compare_columns_sets(query,dropped_query))\n",
    "\n",
    "print(\"For dropped hits\", compare_columns_sets(hits,dropped_hits))\n",
    "\n",
    "if compare_columns_rowwise(dropped_query, 'Created Date', 'Created') != -1:\n",
    "     raise ValueError(compare_columns_rowwise(dropped_query, 'Created Date', 'Created'), \"are the rows where Created Date and Created differ.\")\n",
    "\n",
    "if compare_columns_rowwise(dropped_hits, 'Created Date', 'Created') != -1:\n",
    "    raise ValueError(compare_columns_rowwise(dropped_hits, 'Created Date', 'Created'), \"are the rows where Created Date and Created differ.\")\n",
    "\n",
    "# Combines Created Date into Created\n",
    "dropped_query = combine_col1_into_col2(dropped_query, 'Created Date', 'Created')\n",
    "dropped_hits = combine_col1_into_col2(dropped_hits, 'Created Date', 'Created')\n",
    "print(\"Merged Created Date into Created in both query and hits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e99ad",
   "metadata": {},
   "source": [
    "### Metadata Cleanup\n",
    "\n",
    "The exportation from Geneious Prime has extra data in Name and Query from the sequences in the folders Query and Hits respectively. These will enable us to partially link them together later in analysis.\n",
    "\n",
    "This next block shows exactly what we have to split up in our data, as Geneious prime put more data inside certain cells than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc662f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the peculiar columns in the query and hits.\n",
    "print(\"There's metadata in certain cells.\")\n",
    "# Arbitrary row chosen to demonstrate the metadata.\n",
    "query_val = dropped_hits.loc[10,[\"Query\"]]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"From the Query column in hits:\\n\", query_val)\n",
    "name_val = dropped_query.loc[20,[\"Name\"]]\n",
    "print(\"\\nFrom the Name column in query:\\n\", name_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edfd2d8",
   "metadata": {},
   "source": [
    "Yet, for our queries that were contigs, the \"Name\" does not have the extra data (as it is just named \"Contig #\", so we lose a lot of extra data). We need to split the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b005066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the query table into the contigs and noncontigs. + a TODO\n",
    "# TODO May need to do equals() to see if each column really does correspond to a contig.\n",
    "query_masks = rows_for_peculiar_columns(dropped_query)\n",
    "print(\"These keys are what contigs but not regular sequences have:\\n\", query_masks.keys())\n",
    "contig_query = dropped_query[query_masks['# Source Sequences']]\n",
    "noncontig_query = drop_nan_columns(dropped_query[~query_masks['# Source Sequences']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404b741",
   "metadata": {},
   "source": [
    "Some of the extra data, doesn't actually have extra data, so that also needs to be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits our data up and gets the weird cell split up. Also drops sample_id as there is none.\n",
    "clean_contig_query = contig_query.apply(split_query_name, axis=1) # Nothing happens as Name is just Contig #.\n",
    "clean_noncontig_query = drop_nan_columns(noncontig_query.apply(split_query_name, axis=1))\n",
    "clean_hits = drop_nan_columns(dropped_hits.apply(split_query_name, axis=1, splitting_column=\"Query\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f411024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows unique columns in contigs and non-contigs queries.\n",
    "in_contig, not_in_contig = compare_columns_sets(clean_contig_query, clean_noncontig_query)\n",
    "print(\"This is what is uniquely inside contigs:\\n\", in_contig, \"\\nThis is what is uniquely in non-contigs:\\n\", not_in_contig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840608d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'barcode' and 'barcode_alias' into 'barcode' \n",
    "clean_noncontig_query = combine_col1_into_col2(clean_noncontig_query, 'barcode_alias', 'barcode')\n",
    "clean_hits = combine_col1_into_col2(clean_hits, 'barcode_alias', 'barcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning clean_hits where they are contigs.\n",
    "# This is where we will add the barcode and protocol_group_id.\n",
    "# We will also fill in the parent_read_id with the Query column.\n",
    "\n",
    "clean_hits[['barcode', 'protocol_group_id']] = clean_hits[['barcode', 'protocol_group_id']].bfill().ffill()\n",
    "clean_hits['parent_read_id'] = clean_hits['parent_read_id'].fillna(clean_hits['Query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85547e35",
   "metadata": {},
   "source": [
    "Final look at what the dataframe looks like along with final bug tests before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb35f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in c_hits\n",
    "c_hits_val = clean_hits.loc[1, :]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"From a row in c_hits:\\n\", c_hits_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398479b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in c_contig_query\n",
    "c_contig_val = clean_contig_query.iloc[3,:]\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"Columns (+ examples) in c_contig_query:\\n\")\n",
    "print(\"From a row in c_hits:\\n\", c_contig_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57280492",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO, make tests of data connections.\n",
    "#assert all(hits.columns == query.columns), \"Columns in hits and query do not match\"\n",
    "#assert all(hits.columns == query.columns), \"Columns in hits and query do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594953f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO, analysis of filtered workflow vs unfiltered workflow.\n",
    "#filtered_hits = pd.read_csv(datapath + \"barcode13-filtered-hits.csv\")\n",
    "#filtered_hits = filtered_hits.apply(split_query_name, axis=1)\n",
    "#filtered_hits.columns\n",
    "#hits[hits[\"parent_read_id\"] != filtered_hits[\"parent_read_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e937d264",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "The data has been cleaned up. Now it is time to use that to see the details about the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555baa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful columns in clean_hits (so we can change later):\n",
    "USEFUL_COLS = [\n",
    "    'Name',\n",
    "    'parent_read_id',\n",
    "    'Accession',\n",
    "    'Grade',\n",
    "    'E Value',\n",
    "    'Bit-Score',\n",
    "    '% Identical Sites',\n",
    "    '% Pairwise Identity',\n",
    "    'barcode',\n",
    "    'Sequence Length',\n",
    "]\n",
    "organism_percent = clean_hits['Organism'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets for analysis\n",
    "# Look at https://ipywidgets.readthedocs.io/en/7.7.1/examples/Widget%20Basics.html for more info on widgets.\n",
    "## Dropdown for organism selection\n",
    "organism_dropdown = widgets.Dropdown(\n",
    "    options=list(organism_percent.index),\n",
    "    value=organism_percent.index[0],\n",
    "    description='Organism:',\n",
    "    disabled=False,\n",
    "    continuous_update=False\n",
    ")\n",
    "## IntText for number of results to show\n",
    "num_results_input = widgets.BoundedIntText(\n",
    "    value=3,\n",
    "    min=1,\n",
    "    max=len(clean_hits),\n",
    "    step=1,\n",
    "    description='Top Grade #:',\n",
    "    disabled=False,\n",
    "    continuous_update=False\n",
    ")\n",
    "## Checkbox for ascending grade order\n",
    "ascending_input = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Ascending Grade Order',\n",
    "    disabled=False,\n",
    "    continuous_update=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b211e9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for Analysis\n",
    "## Output for showing results based on Grade from Organism\n",
    "output_organism = widgets.Output(layout={'border': '1px solid black'})\n",
    "def show_top_hits_dropdown(organism_name, n_results, ascending=False):\n",
    "    filtered = clean_hits[clean_hits['Organism'] == organism_name]\n",
    "    try:\n",
    "        filtered = filtered.copy()\n",
    "        filtered['Grade_numeric'] = filtered['Grade'].str.rstrip('%').astype(float)\n",
    "        filtered = filtered.sort_values('Grade_numeric', ascending=ascending)\n",
    "    except Exception:\n",
    "        filtered = filtered.sort_values('Grade', ascending=ascending)\n",
    "    display(filtered.loc[:,USEFUL_COLS].head(n_results))\n",
    "\n",
    "# Updates output based on dropdown change\n",
    "def on_dropdown_change(change):\n",
    "    with output_organism:\n",
    "        clear_output()\n",
    "        show_top_hits_dropdown(organism_dropdown.value, num_results_input.value, ascending_input.value)\n",
    "# Update the output when the num value changes\n",
    "def on_num_results_change(change):\n",
    "    with output_organism:\n",
    "        clear_output()\n",
    "        show_top_hits_dropdown(organism_dropdown.value, num_results_input.value, ascending_input.value)\n",
    "# Update the output when the ascending value changes\n",
    "def on_ascending_input_change(change):\n",
    "    with output_organism:\n",
    "        clear_output()\n",
    "        show_top_hits_dropdown(organism_dropdown.value, num_results_input.value, ascending_input.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de495277",
   "metadata": {},
   "source": [
    "### Number of Unique Organisms Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb822fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial display of top hits for the most common organism\n",
    "for organism, percent in organism_percent.items():\n",
    "    print(f\"{organism} - {percent:.2f} % - {(clean_hits['Organism'] == organism).sum()} hits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67191230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the interactable widget.\n",
    "organism_dropdown.observe(on_dropdown_change, names='value')\n",
    "num_results_input.observe(on_num_results_change, names='value')\n",
    "ascending_input.observe(on_ascending_input_change, names='value')\n",
    "\n",
    "display(organism_dropdown, num_results_input, ascending_input, output_organism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and Dirty Identified vs Nonidentified.\n",
    "print(f\"{clean_hits.shape[0] / ( clean_contig_query.shape[0] + clean_noncontig_query.shape[0]) * 100:.2f}% of sequences were identified.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
